{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data and serializing in pickle archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arquivo = open('dados-animes.csv', encoding='utf-8')\n",
    "linhas = csv.DictReader(arquivo)\n",
    "animes = []\n",
    "synopsis = []\n",
    "for linha in linhas:\n",
    "    dados = {'name': linha['nome'], 'ranked': linha['ranked'], 'synopsis': linha['synopsis']}\n",
    "    animes.append(dados)\n",
    "    synopsis.append(linha['synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava os arquivos serializados\n",
    "# pickle.dump(animes, open('data-animes.pkl', 'wb'))\n",
    "\n",
    "# Lê os arquivos e de-serializa\n",
    "# data = pickle.load(open('data-animes.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Puting animes in numpy array\n",
    "_animes = []\n",
    "for anime in animes:\n",
    "    _animes.append(np.array(anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(animes, open('data-animes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data-animes.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(synopsis[0])  # converts to list of sentences\n",
    "word_tokens = nltk.word_tokenize(synopsis[0])  # converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function - Transform to lowercase and remove punctuations.\n",
    "# Re is a regular expression\n",
    "\n",
    "\n",
    "def preprocessing(line):\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\learning\\ti\\python\\datascience\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Vectorizing the text movies\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a instance of kmeans and fit data\n",
    "kmeans = KMeans(n_clusters=50).fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicting clusters\n",
    "for k, v in enumerate(kmeans.labels_):\n",
    "    data[k]['cluster'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sinopse do anime para pesquisar: Tokyo has become a cruel and merciless city—a place where vicious creatures called “ghouls” exist alongside humans. The citizens of this once great metropolis live in constant fear of these bloodthirsty savages and their thirst for human flesh. However, the greatest threat these ghouls pose is their dangerous ability to masquerade as humans and blend in with society.\n",
      "Cluster: [23]\n"
     ]
    }
   ],
   "source": [
    "# Predicting arguments - Before predict we need to vectorizer the text\n",
    "query = input('Sinopse do anime para pesquisar: ')\n",
    "\n",
    "index = kmeans.predict(tfidf_vectorizer.transform([query]))\n",
    "print(f'Cluster: {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime: Fullmetal Alchemist: Brotherhood - Cluster: 23\n",
      "Anime: Mob Psycho 100 II - Cluster: 23\n",
      "Anime: Mononoke Hime - Cluster: 23\n",
      "Anime: Code Geass: Hangyaku no Lelouch - Cluster: 23\n",
      "Anime: Boku dake ga Inai Machi - Cluster: 23\n",
      "Anime: Mob Psycho 100 - Cluster: 23\n",
      "Anime: Gintama: Yorinuki Gintama-san on Theater 2D - Cluster: 23\n",
      "Anime: Tenkuu no Shiro Laputa - Cluster: 23\n",
      "Anime: Banana Fish - Cluster: 23\n",
      "Anime: Hinamatsuri - Cluster: 23\n",
      "Anime: Trigun - Cluster: 23\n",
      "Anime: xxxHOLiC Rou - Cluster: 23\n",
      "Anime: Detroit Metal City - Cluster: 23\n",
      "Anime: Magi: The Labyrinth of Magic - Cluster: 23\n",
      "Anime: Zankyou no Terror - Cluster: 23\n",
      "Anime: Akira - Cluster: 23\n",
      "Anime: Tokyo Magnitude 8.0 - Cluster: 23\n",
      "Anime: Durarara!!x2 Shou - Cluster: 23\n",
      "Anime: Phantom: Requiem for the Phantom - Cluster: 23\n",
      "Anime: Kekkai Sensen & Beyond - Cluster: 23\n",
      "Anime: Jigoku Shoujo Futakomori - Cluster: 23\n",
      "Anime: Tokyo Ghoul - Cluster: 23\n",
      "Anime: Giant Robo the Animation: Chikyuu ga Seishi Suru Hi - Cluster: 23\n",
      "Anime: Slayers Try - Cluster: 23\n",
      "Anime: Saraiya Goyou - Cluster: 23\n",
      "Anime: Final Fantasy VII: Advent Children - Cluster: 23\n",
      "Anime: Shaman King - Cluster: 23\n",
      "Anime: Slayers - Cluster: 23\n",
      "Anime: K: Missing Kings - Cluster: 23\n",
      "Anime: Dungeon ni Deai wo Motomeru no wa Machigatteiru Darou ka - Cluster: 23\n",
      "Anime: Toaru Kagaku no Railgun - Cluster: 23\n",
      "Anime: Uchuu Kaizoku Captain Herlock - Cluster: 23\n",
      "Anime: Memories - Cluster: 23\n",
      "Anime: Seikai no Senki III - Cluster: 23\n",
      "Anime: Gakuen Alice - Cluster: 23\n",
      "Anime: Texhnolyze - Cluster: 23\n",
      "Anime: Bounen no Xamdou - Cluster: 23\n",
      "Anime: Mondaiji-tachi ga Isekai kara Kuru Sou Desu yo? - Cluster: 23\n",
      "Anime: Elfen Lied - Cluster: 23\n",
      "Anime: Bokura Mada Underground - Cluster: 23\n",
      "Anime: Full Metal Panic! Invisible Victory - Cluster: 23\n"
     ]
    }
   ],
   "source": [
    "# Looking for cluster\n",
    "for k, v in enumerate(data):\n",
    "    if v['cluster'] == index[0]:\n",
    "        print(f\"Anime: {v['name']} - Cluster: {v['cluster']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
